{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATAM Challenge Data Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Descripción del caso\n",
    "\n",
    "El ejercicio de estudio del desafio corresponde al análisis de datos provenientes desde Twitter, actualmente 'X', en donde se busca rescatar información estadistica de los tweets relacionados a las protestas en India de sus granjeros. Estos análisis estan enfocados en dos grandes metricas, la utilización de memoria y tiempo de cada uno de las funciones que buscan entregar respuesta a las preguntas planteadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supuestos y consideraciones\n",
    "\n",
    "Se realizo una revisión inicial a los datos proporcionados, revisando la existencia de tweets duplicados, o si existen más de una mensión a la misma persona en el mismo tweets de la siguiente manera:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import jsonlines\n",
    "\n",
    "data = []\n",
    "file_path = \"../data/farmers-protest-tweets-2021-2-4.json\"\n",
    "with jsonlines.open(file_path) as reader:\n",
    "    for obj in reader:\n",
    "        users = []\n",
    "        users_dedup = []\n",
    "        mentioned_users = obj.get('mentionedUsers', {})\n",
    "        if mentioned_users:\n",
    "            users.extend([item['username'] for item in mentioned_users])\n",
    "            users_dedup.extend(list(set(item['username'] for item in mentioned_users)))\n",
    "        tweet = {\n",
    "            'date': obj.get('date'),\n",
    "            'id': obj.get('id'),\n",
    "            'username': obj.get('user', {}).get('username'),\n",
    "            'mentionedUsers': users,\n",
    "            'users_dedup': users_dedup\n",
    "        }\n",
    "        data.append(tweet)\n",
    "df = pl.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_count = df.group_by('id').len()\n",
    "id_count = id_count.filter(pl.col('len')>1)\n",
    "len(id_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El total de registros duplicados para el dataframe por el id del tweet generado es cero para el archivo de estudio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la revisión de las multiples mensiones en el mismo tweets se veficó de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensiones_duplicadas = df.filter(pl.col('mentionedUsers').len() != pl.col('users_dedup').len())\n",
    "len(mensiones_duplicadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supuestos generales\n",
    "\n",
    "1. Los archivos para su ejecución se encuentran en la ruta: \"../data/farmers-protest-tweets-2021-2-4.json\"\n",
    "2. Los emojis con variaciones de tono de piel serán tratados como emojis distintos.\n",
    "3. Los datos que se extraen desde Twitter son unicos y no tienen duplicados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explicación de las funciones implementadas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "Las top 10 fechas donde hay más tweets. Mencionar el usuario (username) que más publicaciones tiene por cada uno de esos días."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimización de Tiempo\n",
    "\n",
    "Para responder a este requerimiento se optó por utilizar la libreria *polars* de python, con la cual se busca cargar la información a procesar en memoria para posteriormente procesarla según las necesidades del caso.\n",
    "\n",
    "```\n",
    "with jsonlines.open(file_path) as reader:\n",
    "        for obj in reader:\n",
    "            # Se rescata solo los campos que se analizaran\n",
    "            filter_obj = {\n",
    "                'date': obj.get('date'),\n",
    "                'username': obj.get('user', {}).get('username')\n",
    "            }\n",
    "            data.append(filter_obj)\n",
    "```\n",
    "\n",
    "De la estructura de cada objeto json se obtiene la fecha y el username del usuario que realizo el tweet.\n",
    "\n",
    "Luego se procesa con polars, primero para obtener las fechas con más tweets\n",
    "\n",
    "```\n",
    "    top_fechas =( df\n",
    "                    .group_by('date')\n",
    "                    .len()\n",
    "                    .sort('len',descending=True)\n",
    "                    .head(10)['date']\n",
    "                )\n",
    "```\n",
    "\n",
    "Luego se filtra el dataframe inicial por estas fechas\n",
    "\n",
    "```\n",
    "result = (  df_filtrado.group_by(['date', 'username'])\n",
    "            .len() # Se cuenta para cada usuario por fecha \n",
    "            .sort(by=['date', 'len'], descending=[False, True]) # Se ordena por fecha desendente false y largo true\n",
    "            .group_by('date') # se agrupa por fechas\n",
    "            .agg(\n",
    "                    # Se agrega y se rescata le primer usuario por fecha\n",
    "                    pl.col('username').first()\n",
    "                )\n",
    "     )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "from q1_time import q1_time\n",
    "result = q1_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimización de Memoria\n",
    "\n",
    "Para responder a este requerimiento se optó por la lectura secuencial del archivo, por cada linea nueva que se procesa se rescatan las estadisticas necesarias para el cálculo final.\n",
    "\n",
    "```\n",
    "with jsonlines.open(file_path) as reader:\n",
    "    for obj in reader:\n",
    "        # Se obtiene la fecha de cada uno de los registros\n",
    "        fecha_str = obj.get('date')[:10]\n",
    "        # Se castea de string a datetime.date\n",
    "        fecha = datetime.strptime(fecha_str, '%Y-%m-%d').date()\n",
    "        # Se actualiza el contador de fechas\n",
    "        fecha_counter[fecha] += 1\n",
    "        # Se actualiza el contador de usuarios por fechas\n",
    "        fecha_usuario_counter[fecha][obj.get('user', {}).get('username')] += 1\n",
    "```\n",
    "\n",
    "Se utiliza un  `fecha_usuario_counter = defaultdict(Counter)` para almacenar los totales por cada fecha y usuario.\n",
    "\n",
    "Luego se itera por cada una de las fechas.\n",
    "\n",
    "```\n",
    "for fecha in top_fechas:\n",
    "    usuarios = fecha_usuario_counter[fecha[0]]\n",
    "    usuario_mas_repetido = usuarios.most_common(1)[0][0]\n",
    "    result.append((fecha[0], usuario_mas_repetido))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "from q1_memory import q1_memory\n",
    "result = q1_memory(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de tiempo y uso de memoria para cada una de las funciones\n",
    "#### Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4391046240000005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "from q1_time import q1_time\n",
    "from q1_memory import q1_memory\n",
    "\n",
    "output_stream = StringIO()\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "q1_time(file_path)\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.stream = output_stream\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('time')\n",
    "stats.print_stats()\n",
    "\n",
    "# Obtener el tottime final\n",
    "tiempo_total_q1_time = stats.total_tt\n",
    "tiempo_total_q1_time\n",
    "\n",
    "output_stream = StringIO()\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "q1_memory(file_path)\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.stream = output_stream\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('time')\n",
    "stats.print_stats()\n",
    "# Obtener el tottime final\n",
    "tiempo_total_q1_memory = stats.total_tt\n",
    "tiempo_total_q1_memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La funcion que prioriza el tiempo tiene un consumo de: 2.4391 segundos, \n",
      "mientras que la funcion que prioriza el uso de memoria tiene un consumo de: 3.1812 segundos\n",
      "Lo que representra una mejora de 30.4233%\n"
     ]
    }
   ],
   "source": [
    "print(f'La funcion que prioriza el tiempo tiene un consumo de: {tiempo_total_q1_time:.4f} segundos, \\nmientras que la funcion que prioriza el uso de memoria tiene un consumo de: {tiempo_total_q1_memory:.4f} segundos')\n",
    "diferencia_porcentual = (abs(tiempo_total_q1_memory - tiempo_total_q1_time) / tiempo_total_q1_time) * 100\n",
    "print(f'Lo que representra una mejora de {diferencia_porcentual:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/hvera/Dev/DE_LATAM_challenge/LATAM-Data-Engineer-Challenge/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     6     96.1 MiB     96.1 MiB           1   def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     7                                             \"\"\"\n",
      "     8                                             Funcion que lee un archivo JSONL y encuentra las 10 fechas con mayor cantidad de tweets, para cada una de estas fechas entrega el usuario con mayor cantidad de tweets.\n",
      "     9                                             \n",
      "    10                                             Args:\n",
      "    11                                                 file_path (str): Ruta del archivo JSONL que contiene los tweets para analizar.\n",
      "    12                                             \n",
      "    13                                             Returns:\n",
      "    14                                                 List[Tuple[datetime.date, str]]: Lista de tuplas, donde el primer elemento corresponde a la fecha y el segundo elemento al nombre del usuario.\n",
      "    15                                             \"\"\"\n",
      "    16     96.1 MiB      0.0 MiB           1       result = []\n",
      "    17                                             # Contador para las fechas\n",
      "    18     96.1 MiB      0.0 MiB           1       fecha_counter = Counter()\n",
      "    19     96.1 MiB      0.0 MiB           1       fecha_usuario_counter = defaultdict(Counter)\n",
      "    20                                             # Lectura de archivo JSONL, se lee linea por linea para para reducir el uso de memoria\n",
      "    21     96.2 MiB      0.0 MiB           1       with jsonlines.open(file_path) as reader:\n",
      "    22     98.3 MiB -10890.4 MiB      117408           for obj in reader:\n",
      "    23                                                     # Se obtiene la fecha de cada uno de los registros\n",
      "    24     98.3 MiB -10890.2 MiB      117407               fecha_str = obj.get('date')[:10]\n",
      "    25                                                     # Se castea de string a datetime.date\n",
      "    26     98.3 MiB -10891.3 MiB      117407               fecha = datetime.strptime(fecha_str, '%Y-%m-%d').date()\n",
      "    27                                                     # Se actualiza el contador de fechas\n",
      "    28     98.3 MiB -10891.4 MiB      117407               fecha_counter[fecha] += 1\n",
      "    29                                                     # Se actualiza el contador de usuarios por fechas\n",
      "    30     98.3 MiB -10889.9 MiB      117407               fecha_usuario_counter[fecha][obj.get('user', {}).get('username')] += 1\n",
      "    31                                             \n",
      "    32                                             # El top 10 de elementos mas comunes del contador\n",
      "    33     97.3 MiB     -0.9 MiB           1       top_fechas = fecha_counter.most_common(10)\n",
      "    34                                             # Se itera por cada una de las fechas con mayor cantidad de tweets\n",
      "    35     97.4 MiB      0.0 MiB          11       for fecha in top_fechas:\n",
      "    36     97.4 MiB      0.0 MiB          10           usuarios = fecha_usuario_counter[fecha[0]]\n",
      "    37     97.4 MiB      0.0 MiB          10           usuario_mas_repetido = usuarios.most_common(1)[0][0]\n",
      "    38     97.4 MiB      0.0 MiB          10           result.append((fecha[0], usuario_mas_repetido))    \n",
      "    39                                             \n",
      "    40     97.4 MiB      0.0 MiB           1       return result"
     ]
    }
   ],
   "source": [
    "%reload_ext memory_profiler\n",
    "%mprun -f q1_memory q1_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/hvera/Dev/DE_LATAM_challenge/LATAM-Data-Engineer-Challenge/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     6     97.1 MiB     97.1 MiB           1   def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     7                                             \"\"\"\n",
      "     8                                             Funcion que lee un archivo JSONL y encuentra las 10 fechas con mayor cantidad de tweets, para cada una de estas fechas entrega el usuario con mayor cantidad de tweets, se prioriza el tiempo de ejecucion.\n",
      "     9                                             Para esto se carga toda la informacion en memoria antes de procesarla.\n",
      "    10                                             Args:\n",
      "    11                                                 file_path (str): Ruta del archivo JSONL que contiene los tweets para analizar.\n",
      "    12                                             \n",
      "    13                                             Returns:\n",
      "    14                                                 List[Tuple[datetime.date, str]]: Lista de tuplas, donde el primer elemento corresponde a la fecha y el segundo elemento al nombre del usuario.\n",
      "    15                                             \"\"\"\n",
      "    16                                             # Lista que almacena la informacion en memoria linea por linea\n",
      "    17     97.1 MiB      0.0 MiB           1       data = []\n",
      "    18                                             # Lectura de archivo JSONL, se lee linea por linea\n",
      "    19     97.2 MiB      0.0 MiB           1       with jsonlines.open(file_path) as reader:\n",
      "    20    135.4 MiB  -1369.9 MiB      117408           for obj in reader:\n",
      "    21                                                     # Se rescata solo los campos que se analizaran\n",
      "    22    135.3 MiB  -1408.2 MiB      117407               filter_obj = {\n",
      "    23    135.3 MiB  -1407.6 MiB      117407                   'date': obj.get('date'),\n",
      "    24    135.3 MiB  -1407.7 MiB      117407                   'username': obj.get('user', {}).get('username')\n",
      "    25                                                     }\n",
      "    26    135.3 MiB  -1408.0 MiB      117407               data.append(filter_obj)\n",
      "    27                                             # creacion de dataframe\n",
      "    28    162.5 MiB     27.1 MiB           1       df = pl.DataFrame(data)\n",
      "    29                                             # Transformacion de la columna date de str a date\n",
      "    30    164.7 MiB      2.3 MiB           1       df = df.with_columns([pl.col('date').str.strptime(pl.Date, \"%Y-%m-%dT%H:%M:%S%z\").alias('date')])\n",
      "    31                                             \n",
      "    32                                             # Se retornan las 10 fechas con mayor cantidad de tweets\n",
      "    33    169.4 MiB      4.6 MiB           5       top_fechas =( df\n",
      "    34    164.8 MiB      0.0 MiB           1                       .group_by('date')\n",
      "    35                                                             .len()\n",
      "    36    169.2 MiB      0.0 MiB           1                       .sort('len',descending=True)\n",
      "    37    169.2 MiB      0.0 MiB           2                       .head(10)['date']\n",
      "    38                                                         )\n",
      "    39                                             # Se filtra el dataset por estas fechas\n",
      "    40    172.6 MiB      3.3 MiB           1       df_filtrado = df.filter(pl.col('date').is_in(top_fechas))\n",
      "    41                                             # Se elimina el dataset original\n",
      "    42    172.6 MiB      0.0 MiB           1       del df\n",
      "    43                                             # Se procesa el dataset filtrado\n",
      "    44    181.8 MiB      9.1 MiB           4       result = (  df_filtrado.group_by(['date', 'username'])\n",
      "    45                                                     .len() # Se cuenta para cada usuario por fecha \n",
      "    46    180.5 MiB      0.0 MiB           1               .sort(by=['date', 'len'], descending=[False, True]) # Se ordena por fecha desendente false y largo true\n",
      "    47    181.5 MiB      0.0 MiB           1               .group_by('date') # se agrupa por fechas\n",
      "    48                                                     .agg(\n",
      "    49                                                             # Se agrega y se rescata le primer usuario por fecha\n",
      "    50    181.6 MiB      0.0 MiB           1                       pl.col('username').first()\n",
      "    51                                                         )\n",
      "    52                                              )\n",
      "    53                                             # El resultado se joinea con las fechas para mantener el orden original\n",
      "    54                                             # que consiste en las fechas con mas tweets\n",
      "    55    182.5 MiB      0.7 MiB           1       result = top_fechas.to_frame().join(result, on='date',how=\"left\")\n",
      "    56                                             # Se formatea el output para que sea consistente con la descripcion de la funcion\n",
      "    57    182.5 MiB      0.1 MiB          13       result = [tuple(row.values()) for row in result.to_dicts()]\n",
      "    58    182.5 MiB      0.0 MiB           1       return result"
     ]
    }
   ],
   "source": [
    "%reload_ext memory_profiler\n",
    "%mprun -f q1_time q1_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "q1_memory.py    26.4 MiB\n",
    "q1_time.py      100.0 MiB\n",
    "q2_memory.py    26.2 MiB\n",
    "q2_time.py      1660.4 MiB\n",
    "q3_memory.py    22.3 MiB\n",
    "q3_time.py      64.1 MiB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "Los top 10 emojis más usados con su respectivo conteo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimización de Tiempo\n",
    "\n",
    "Para responder a este requerimiento se optó por utilizar la libreria *pandas* de python, a diferencia del resto de implementaciones se cargo todo el archivo en memoria para luego procesarlo.\n",
    "\n",
    "`df = pd.read_json(file_path, lines=True)`\n",
    "\n",
    "Luego utilizando la libreria *emot* se utiliza su implementación para la lectura con multiprocesador en la siguiente linea:\n",
    "\n",
    "`emojis = emot_obj.bulk_emoji(data)`\n",
    "\n",
    "Para luego continuar con el procesamiento y terminar contando con *Counter*.\n",
    "\n",
    "Como mejoras a este ejercicio se propone identificar emojis con cambio de tono de piel como un solo emoji, por ejemplo: 👍🏽 y 👍, sean tratados como un solo emoji y no dos como esta en la implementación actual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 5049), ('😂', 3072), ('🚜', 2972), ('🌾', 2182), ('🇮🇳', 2086), ('❤', 1779), ('🤣', 1668), ('✊', 1651), ('🙏🏻', 1317), ('💚', 1040)]\n"
     ]
    }
   ],
   "source": [
    "from q2_time import q2_time\n",
    "result = q2_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimización de Memoria\n",
    "\n",
    "Para responder a este requerimiento se optó por la lectura secuencial del archivo utilizando la libreria *jsonlines*.\n",
    "\n",
    "`with jsonlines.open(file_path) as reader`\n",
    "\n",
    "Por cada una de las lineas procesadas se utiliza el metodo *emoji* que retorna un diccionario con los emojis y su ubicación en el texto.\n",
    "\n",
    "`emojis = emot_obj.emoji(content)`\n",
    "\n",
    "Ejemplo de salida:\n",
    "\n",
    "` {'value': ['☮', '🙂', '❤'], 'location': [[14, 15], [16, 17], [18, 19]], 'mean': [':peace_symbol:',':slightly_smiling_face:', ':red_heart:'], 'flag': True}` \n",
    "\n",
    "De este resultado se utilza 'value' el que se agrega al contador\n",
    "\n",
    "`emojis = [item for item in emojis['value']]`\n",
    "\n",
    "`contador.update(emojis)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 5049), ('😂', 3072), ('🚜', 2972), ('🌾', 2182), ('🇮🇳', 2086), ('❤', 1779), ('🤣', 1668), ('✊', 1651), ('🙏🏻', 1317), ('💚', 1040)]\n"
     ]
    }
   ],
   "source": [
    "from q2_memory import q2_memory\n",
    "result = q2_memory(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de tiempo y uso de memoria para cada una de las funciones\n",
    "#### Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "from q2_time import q2_time\n",
    "from q2_memory import q2_memory\n",
    "\n",
    "output_stream = StringIO()\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "q2_time(file_path)\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.stream = output_stream\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('time')\n",
    "stats.print_stats()\n",
    "\n",
    "# Obtener el tottime final\n",
    "tiempo_total_q2_time = stats.total_tt\n",
    "\n",
    "output_stream = StringIO()\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "q2_memory(file_path)\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.stream = output_stream\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('time')\n",
    "stats.print_stats()\n",
    "# Obtener el tottime final\n",
    "tiempo_total_q2_memory = stats.total_tt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La funcion que prioriza el tiempo tiene un consumo de: 10.1644 segundos, \n",
      "mientras que la funcion que prioriza el uso de memoria tiene un consumo de: 19.8816 segundos\n",
      "Lo que representra una mejora de 95.6000%\n"
     ]
    }
   ],
   "source": [
    "print(f'La funcion que prioriza el tiempo tiene un consumo de: {tiempo_total_q2_time:.4f} segundos, \\nmientras que la funcion que prioriza el uso de memoria tiene un consumo de: {tiempo_total_q2_memory:.4f} segundos')\n",
    "diferencia_porcentual = (abs(tiempo_total_q2_memory - tiempo_total_q2_time) / tiempo_total_q2_time) * 100\n",
    "print(f'Lo que representra una mejora de {diferencia_porcentual:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/hvera/Dev/DE_LATAM_challenge/LATAM-Data-Engineer-Challenge/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     6    182.1 MiB    182.1 MiB           1   def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     7                                             \"\"\"\n",
      "     8                                             Funcion que lee un archivo JSONL y encuentra los 10 emojis mas usados, priorizando el uso de memoria.\n",
      "     9                                             \n",
      "    10                                             Args:\n",
      "    11                                                 file_path (str): Ruta del archivo JSONL que contiene los tweets para analizar.\n",
      "    12                                             \n",
      "    13                                             Returns:\n",
      "    14                                                 List[Tuple[str, int]]: Lista de tuplas, donde el primer elemento corresponde al emoji y el segundo a la cantidad de veces que se utiliza.\n",
      "    15                                             \"\"\"\n",
      "    16                                             # Contador para los emojis\n",
      "    17    182.1 MiB      0.0 MiB           1       contador = Counter()\n",
      "    18                                             # Se genera un objeto emot.core.emot\n",
      "    19    183.6 MiB      1.5 MiB           1       emot_obj = emot.core.emot()\n",
      "    20                                             # Lectura de archivo JSONL, se lee linea por linea para para reducir el uso de memoria\n",
      "    21    183.6 MiB      0.0 MiB           1       with jsonlines.open(file_path) as reader:\n",
      "    22    183.9 MiB -17803.5 MiB      117408           for obj in reader:\n",
      "    23                                                     # Se obtiene el objeto content que es donde esta el cuerpo del tweet\n",
      "    24    183.9 MiB -17803.2 MiB      117407               content = obj.get('content')\n",
      "    25                                                     # Se obtiene la lista de diccionarios de los emojis encontrados en el texto\n",
      "    26    183.9 MiB -17803.4 MiB      117407               emojis = emot_obj.emoji(content)\n",
      "    27    183.9 MiB -17803.6 MiB      117407               if(emojis['value']):\n",
      "    28                                                         # Si se encontraron emojis en el texto se almacena solo los emojis\n",
      "    29    183.9 MiB -14560.1 MiB       93534                   emojis = [item for item in emojis['value']]\n",
      "    30                                                         # Se actualiza el contador con la lista de emojis\n",
      "    31    183.9 MiB  -2616.0 MiB       16869                   contador.update(emojis)\n",
      "    32                                             # Se retorna el top 10 de emojis\n",
      "    33    183.6 MiB     -0.3 MiB           1       return contador.most_common(10)"
     ]
    }
   ],
   "source": [
    "%reload_ext memory_profiler\n",
    "%mprun -f q2_memory q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/hvera/Dev/DE_LATAM_challenge/LATAM-Data-Engineer-Challenge/src/q2_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8    182.8 MiB    182.8 MiB           1   def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "     9                                             \"\"\"\n",
      "    10                                             Funcion que lee un archivo JSONL y encuentra los 10 emojis mas usados, priorizando el uso de memoria.\n",
      "    11                                         \n",
      "    12                                             Args:\n",
      "    13                                                 file_path (str): Ruta del archivo JSONL que contiene los tweets para analizar.\n",
      "    14                                         \n",
      "    15                                             Returns:\n",
      "    16                                                 List[Tuple[str, int]]: Lista de tuplas, donde el primer elemento corresponde al emoji y el segundo a la cantidad de veces que se utiliza.\n",
      "    17                                             \"\"\"\n",
      "    18                                             # Contador para los emojis\n",
      "    19    182.8 MiB      0.0 MiB           1       contador = Counter()\n",
      "    20                                             # Lectura de archivo JSONL, se carga todo en memoria con pandas\n",
      "    21   1857.6 MiB   1674.8 MiB           1       df = pd.read_json(file_path, lines=True)\n",
      "    22                                             # Se genera un objeto emot.core.emot\n",
      "    23   1858.8 MiB      1.2 MiB           1       emot_obj = emot.core.emot()\n",
      "    24                                             # Se genera una lista con todos los elementos del dataframe de la columna content\n",
      "    25                                             # Esta columna es la que tiene la informacion del texto del tweet\n",
      "    26   1858.8 MiB      0.0 MiB           1       data = df['content'].tolist()\n",
      "    27                                             # Se utiliza el metodo bulk_emoji ya que utiliza multi procesamiento\n",
      "    28   1901.8 MiB     43.0 MiB           1       emojis = emot_obj.bulk_emoji(data)\n",
      "    29                                             # Se deja la lista aplana la lista de listas solamente cuando existen emojis\n",
      "    30   1901.8 MiB      0.0 MiB      151149       emojis = list(chain.from_iterable(item['value']\n",
      "    31   1901.8 MiB      0.0 MiB      117408                     for item in emojis if item['value']))\n",
      "    32                                             # Actualizo el contador\n",
      "    33   1901.8 MiB      0.0 MiB           1       contador.update(emojis)\n",
      "    34   1901.8 MiB      0.0 MiB           1       return contador.most_common(10)"
     ]
    }
   ],
   "source": [
    "%reload_ext memory_profiler\n",
    "%mprun -f q2_time q2_time(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "El top 10 histórico de usuarios (username) más influyentes en función del conteo de las menciones (@) que registra cada uno de ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimización de Tiempo\n",
    "\n",
    "Para responder a este requerimiento se utilizó la misma aproximación que para el ejercicio 1, en donde se lee el archivo linea por linea con *jsonlines* para de esta forma generar un consilidado de los usuarios mencionados.\n",
    "\n",
    "```\n",
    "with jsonlines.open(file_path) as reader:\n",
    "    for obj in reader:\n",
    "        mentioned_users = obj.get('mentionedUsers', {})\n",
    "        if mentioned_users:\n",
    "            data.extend([item['username'] for item in mentioned_users])\n",
    "```\n",
    "\n",
    "En la lista data se almacenan todos los usuarios que aparecen como menciones.\n",
    "Luego se genera un dataframe de *polars* para contar, ordernar y retornar los 10 elementos más comunes.\n",
    "\n",
    "```\n",
    "result = (df\n",
    "            .group_by('column_0') # se agrupa por la column_0, que es el username\n",
    "            .len() # Se cuentan los registros\n",
    "            .sort('len',descending=True) # Se ordenan de manera descendente\n",
    "            .head(10) # se retornan los 10 primeros registros\n",
    "   )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "from q3_time import q3_time\n",
    "result = q3_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimización de Memoria\n",
    "\n",
    "Para responder a este requerimiento se optó por la lectura secuencial del archivo, al igual que la implementación del ejercicio 1 por cada linea nueva que se procesa se rescatan las estadisticas necesarias para el cálculo final necesario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "from q3_memory import q3_memory\n",
    "result = q3_memory(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de tiempo y uso de memoria para cada una de las funciones\n",
    "#### Tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "from q3_time import q3_time\n",
    "from q3_memory import q3_memory\n",
    "\n",
    "output_stream = StringIO()\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "q3_time(file_path)\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.stream = output_stream\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('time')\n",
    "stats.print_stats()\n",
    "\n",
    "# Obtener el tottime final\n",
    "tiempo_total_q3_time = stats.total_tt\n",
    "\n",
    "output_stream = StringIO()\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "q3_memory(file_path)\n",
    "profiler.disable()\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.stream = output_stream\n",
    "stats.strip_dirs()\n",
    "stats.sort_stats('time')\n",
    "stats.print_stats()\n",
    "# Obtener el tottime final\n",
    "tiempo_total_q3_memory = stats.total_tt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La funcion que prioriza el tiempo tiene un consumo de: 2.3327 segundos, \n",
      "mientras que la funcion que prioriza el uso de memoria tiene un consumo de: 2.3663 segundos\n",
      "Lo que representra una mejora de 1.4403%\n"
     ]
    }
   ],
   "source": [
    "print(f'La funcion que prioriza el tiempo tiene un consumo de: {tiempo_total_q3_time:.4f} segundos, \\nmientras que la funcion que prioriza el uso de memoria tiene un consumo de: {tiempo_total_q3_memory:.4f} segundos')\n",
    "diferencia_porcentual = (abs(tiempo_total_q3_memory - tiempo_total_q3_time) / tiempo_total_q3_time) * 100\n",
    "print(f'Lo que representra una mejora de {diferencia_porcentual:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/hvera/Dev/DE_LATAM_challenge/LATAM-Data-Engineer-Challenge/src/q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     5    509.6 MiB    509.6 MiB           1   def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     6                                             \"\"\"\n",
      "     7                                             Funcion que lee un archivo JSONL y encuentra los 10 usuarios con mayor numero de menciones.\n",
      "     8                                             \n",
      "     9                                             Args:\n",
      "    10                                                 file_path (str): Ruta del archivo JSONL que contiene los tweets para analizar.\n",
      "    11                                             \n",
      "    12                                             Returns:\n",
      "    13                                                 List[Tuple[str, int]]: Lista de tuplas, donde el primer elemento corresponde al nombre de usuario y el segundo a la cantidad de menciones.\n",
      "    14                                             \"\"\"\n",
      "    15                                             # Contador para los usuarios\n",
      "    16    509.6 MiB      0.0 MiB           1       contador = Counter()\n",
      "    17                                             # Lectura de archivo JSONL, se lee linea por linea para para reducir el uso de memoria\n",
      "    18    509.6 MiB      0.0 MiB           1       with jsonlines.open(file_path) as reader:\n",
      "    19    509.6 MiB      0.0 MiB      117408           for obj in reader:\n",
      "    20                                                     # Se rescata solo el campo mentionedUsers\n",
      "    21    509.6 MiB      0.0 MiB      117407               mentioned_users = obj.get('mentionedUsers', {})\n",
      "    22    509.6 MiB      0.0 MiB      117407               if mentioned_users:\n",
      "    23                                                         # Si existen usuarios mencionados rescato la lista de username\n",
      "    24    509.6 MiB      0.0 MiB      217505                   mentioned_users = [item['username'] for item in mentioned_users]\n",
      "    25                                                         # Actualizo con contador de usuarios\n",
      "    26    509.6 MiB      0.0 MiB       38034                   contador.update(mentioned_users)\n",
      "    27    509.6 MiB      0.0 MiB           1       return contador.most_common(10)"
     ]
    }
   ],
   "source": [
    "%reload_ext memory_profiler\n",
    "%mprun -f q3_memory q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/hvera/Dev/DE_LATAM_challenge/LATAM-Data-Engineer-Challenge/src/q3_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     5    508.6 MiB    508.6 MiB           1   def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "     6                                             \"\"\"\n",
      "     7                                             Funcion que lee un archivo JSONL y encuentra los 10 usuarios con mayor numero de menciones, prioriza el tiempo de ejecucion.\n",
      "     8                                             \n",
      "     9                                             Args:\n",
      "    10                                                 file_path (str): Ruta del archivo JSONL que contiene los tweets para analizar.\n",
      "    11                                             \n",
      "    12                                             Returns:\n",
      "    13                                                 List[Tuple[str, int]]: Lista de tuplas, donde el primer elemento corresponde al nombre de usuario y el segundo a la cantidad de menciones.\n",
      "    14                                             \"\"\"\n",
      "    15                                         \n",
      "    16                                             # Lista que almacena la informacion en memoria linea por linea\n",
      "    17    508.6 MiB      0.0 MiB           1       data = []\n",
      "    18    508.6 MiB      0.0 MiB           1       result = []\n",
      "    19                                             # Lectura de archivo JSONL, se lee linea por linea\n",
      "    20    508.6 MiB      0.0 MiB           1       with jsonlines.open(file_path) as reader:\n",
      "    21    508.7 MiB      0.0 MiB      117408           for obj in reader:\n",
      "    22                                                     # Se rescata solo el campo mentionedUsers\n",
      "    23    508.7 MiB      0.0 MiB      117407               mentioned_users = obj.get('mentionedUsers', {})\n",
      "    24                                                     # Si se tienen mensiones en el tweet\n",
      "    25    508.7 MiB      0.0 MiB      117407               if mentioned_users:\n",
      "    26                                                         # Se extraen todos los usuarios mensionados\n",
      "    27    508.7 MiB      0.0 MiB      217505                   data.extend([item['username'] for item in mentioned_users])\n",
      "    28                                             # Creacion de dataframe\n",
      "    29    512.9 MiB      4.2 MiB           1       df = pl.DataFrame(data)\n",
      "    30                                             # Analisis del dataframe\n",
      "    31    523.9 MiB     11.1 MiB           4       result = (df\n",
      "    32    512.9 MiB      0.0 MiB           1                   .group_by('column_0') # se agrupa por la column_0, que es el username\n",
      "    33                                                         .len() # Se cuentan los registros\n",
      "    34    523.0 MiB      0.0 MiB           1                   .sort('len',descending=True) # Se ordenan de manera descendente\n",
      "    35    523.9 MiB      0.0 MiB           1                   .head(10) # se retornan los 10 primeros registros\n",
      "    36                                             )\n",
      "    37                                             # Se formatea el output para que sea consistente con la descripcion de la funcion\n",
      "    38    524.1 MiB      0.1 MiB          13       result = [tuple(row.values()) for row in result.to_dicts()]\n",
      "    39    524.1 MiB      0.0 MiB           1       return result"
     ]
    }
   ],
   "source": [
    "%reload_ext memory_profiler\n",
    "%mprun -f q3_time q3_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
